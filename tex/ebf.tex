\documentclass[11pt,xcolor=svgnames]{beamer}
\usepackage{dsfont,natbib,setspace,changepage,multirow}
\mode<presentation>

% replaces beamer foot with simple page number
\setbeamertemplate{navigation symbols}{}
%\setbeamerfont{frametitle}{series=\bfseries}
\setbeamercolor{frametitle}{fg=Black}

\setbeamertemplate{footline}{
   \raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[20pt]{\color{gray}\scriptsize\insertframenumber}}}}

\graphicspath{{/Users/mtaddy/Dropbox/inputs/}}
\usepackage{algorithm}
\usepackage{algorithmic}

% colors
\newcommand{\theme}{\color{Maroon}}
\newcommand{\bk}{\color{black}}
\newcommand{\rd}{\color{DarkRed}}
\newcommand{\fg}{\color{ForestGreen}}
\newcommand{\bl}{\color{blue}}
\newcommand{\gr}{\color{black!50}}
\newcommand{\sg}{\color{DarkSlateGray}}
\newcommand{\nv}{\color{Navy}}
\setbeamercolor{itemize item}{fg=gray}

% common math markups
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\bm}[1]{\mathbf{#1}}
\newcommand{\ds}[1]{\mathds{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\def\plus{\texttt{+}}
\def\minus{\texttt{-}}

% spacing and style shorthand
\setstretch{1.1}

\begin{document}

\setcounter{page}{0}
{% \usebackgroundtemplate{\includegraphics[height=\paperheight]{phoenix}}
\begin{frame}[plain]
\begin{center}

{\bf \LARGE \theme }

\vskip .25cm
{\bf \LARGE \theme Empirical Bayesian Forests }

\vskip 1cm
Matt Taddy,  Chicago Booth

\vskip .1cm
with Chun-Sheng Chen, Jun Yun, and Mitch Wyle at eBay


\vfill
{\texttt{faculty.chicagobooth.edu/matt.taddy/research}}

\end{center}
\end{frame} }


\begin{frame}
{What is a Decision Tree?}

\includegraphics[width=4.25in]{../../bigdata/graphs/umbrella}


\vskip .3cm
\bk Tree-logic uses a series of steps to come to a conclusion.\\
 The trick is to have mini-decisions combine for good choices.
\\ Each decision is a node, and the final prediction is a {\theme
  leaf node}

\end{frame}


\begin{frame}

{\bf \theme Neighborhoods: \bk Parents, Siblings, Children, and Leaves}

\vskip -.25cm
\begin{columns}

\column{1.75in}

\vskip -.1cm
\begin{equation*}~~~~
\begin{array}{ccccc}
& &  \hskip -1cm P(\eta) &&\\
& & \hskip -1cm \swarrow ~~~\searrow &&\\
&  \hskip -.5cm\eta & & \hskip -.5cm S(\eta)&\\
& \hskip -.5cm \swarrow~~~\searrow & & &\\
C_{l}(\eta) & & \hskip -.3cm  C_r(\eta) & &
\end{array}
\end{equation*}

\column{2.75in}

\vskip .5cm

Every node has a {\it parent} (except root).

\vskip .5cm
Nodes sharing  parents are {\it siblings}.

\vskip .5cm
Each node can have its own {\it children}.
\end{columns}


\vskip .5cm
{After descending through splits, {\nv leaf nodes} hold a
  data subset}

\vskip -.25cm
\begin{equation*}
\begin{array}{ccccc}
& &  \hskip -2.2cm x_i = 0 &&\\
& & \hskip -2.3cm \swarrow ~~~\searrow &&\\
&  \hskip -.8cm x_j = 2 & & \hskip -2.3cm \{\bm{x}: x_i>0\}&\\
& \hskip -.7cm \swarrow~~~\searrow & & &\\
\{\bm{x}: x_i \leq  0, x_j \leq  2\}& & \hskip -.5cm \{ \bm{x}: x_i \leq 0, x_j>2 \}& &
\end{array}
\end{equation*}

\vskip -.5cm
\end{frame}

\begin{frame}


{\bf  Estimation of Decision Trees}

\vskip .5cm  We maximize data likelihood 
   by thinking recursively: 

~~~~~ \bk Split the data into two different decisions about $y$. 

\hfill Take each new partition and split again.

\vskip .5cm
\bk
{\bf \theme Growing \bk your tree  with the \theme CART \bk
  algorithm:}

\begin{itemize}\bk
\item Find the split location in $\bm{x}$ that minimizes deviance.
\begin{itemize}
\item[\gr $\bullet$] $\hat{y}_i$ or $\hat{p}_i$ change depending on whether $x_i <
  x_{\text{split}}$.
\end{itemize}
\item You then grow the tree at this point
\begin{itemize}
\item[\gr $\bullet$]  Each new child node contains a subset of the data.
\end{itemize}
\item View each child as a new dataset, and try to grow again.
\begin{itemize}
\item[\gr $\bullet$] Stop splitting/growing when there are some fixed minimum 
number of observations in each leaf node. 
\end{itemize}
\end{itemize}

\vskip -.25cm
\end{frame}


\begin{frame}

{\bf \theme Random Forests}

\vskip .5cm \bk
CART is an effective way to choose a single tree, but often there are
many possible trees that fit the data similarly well.


\vskip .5cm \nv
An alternative approach is to make use of {\nv random forests}.

\bk
\vskip .25cm
$\bullet$ Sample $B$ subsets of the data {\tt +} variables: \\ ~~~e.g., 
observations $1,5,20,...$ and inputs $2,10,17,...$

\vskip .2cm
$\bullet$ Fit a tree to each subset, to get $B$ fitted trees is $\mc{T}_b$.

\vskip .2cm
$\bullet$
Average prediction across trees:

\vskip .1cm
 ~~~~ -  for regression average $\ds{E}[y|\bm{x}] = \frac{1}{B}\sum_{b=1}^B \mc{T}_b(\bm{x})$.

 \vskip .1cm
 ~~~~ -  for classification let $\{\mc{T}_b(\bm{x})\}_{b=1}^B$ vote on $\hat y$.

\vskip .25cm
{\gr The observation resample is usually {\it with-replacement}, so that this is taking the {\it average of bootstrapped trees} (i.e., `bagging')}

\end{frame}

\begin{frame}

{\theme \bf `distribution free' nonparametric statistics }

\vskip .25cm
1: Find some statistic that matters for your problem, \\~~~~regardless of the `data generating process' (DGP).

\vskip .25cm
2: Derive the distribution for this stat under minimal assumptions.

\vskip .5cm
For (2): say $\bm{z}_l = \{\bm{x}_l,y_l\}$ is a possible data point. Then
\[
\mr{p}(\bm{Z}) = \frac{1}{|\bs{\theta}|}\sum_{l=1}^L \theta_l \ds{1}{[\bm{Z} = \bm{z}_l]}
\]
where $L$ is a {\it large} number of possible values.
\begin{itemize}
\item Use sample as stand-in for the $L$  points.
\item Bayesian  model for the $\theta_l$ weights.
\item {\it This is essentially a bootstrap.}
\end{itemize}

\end{frame}

\begin{frame}[fragile]


Our statistic of interest is the CART fit...

\vskip .5cm
{\sc Bayesian Forest}
\begin{algorithmic}
   \FOR{$b=1$ {\bfseries to} $B$}
   \STATE draw $\boldsymbol{\theta}^b \stackrel{iid}{\sim} \mathrm{Exp}(\mathbf{1})$
   \STATE run weighted-sample CART to get $\mathcal{T}_b = \mathcal{T}(\boldsymbol{\theta}^b)$
   \ENDFOR
\end{algorithmic}

\vskip .5cm
\includegraphics[width=.95\textwidth]{../graphs/mcycle}   

\end{frame}

\begin{frame}


Given forests as a posterior, we can start talking about {\it variance}.

\vskip .5cm
Consider data at a given node in full-sample CART.

\vskip .1cm
Say 
$\mu_j(x_{ij})$ is the leaf rule implied by next splitting on a given $x_j$. 

\vskip .1cm
The resulting {\it impurity is a posterior functional}:
\begin{equation*}
\sigma^2_j(\boldsymbol{\theta}) = \frac{1}{n}\sum_i \theta_i \left[y_i - \mu_j(x_{ij})\right]^2.
\end{equation*}

\vskip .3cm
Say $\sigma_1(\boldsymbol{\theta})$ is the full CART split's impurity, \\and $\sigma_j(\boldsymbol{\theta})$  is that for any other location.

\vskip .5cm
Then we can derive a first order approx to $\Delta_j(\boldsymbol{\theta})   = \sigma_1^2(\boldsymbol{\theta}) - \sigma_j^2(\boldsymbol{\theta})$

\vskip .2cm
\hfill {\theme and we then have $\mathrm{p}(\Delta_j) < 0$.}

\end{frame}

\begin{frame}

{\bf Theoretical trunk stability}

\vskip .5cm
For the data at a given node on the
sample CART tree, the probability that the next split for a posterior
DGP realization  matches the sample split location is
\begin{equation*}
\mathrm{p}\left(\text{split matches sample CART}\right) \gtrsim 1 - \frac{p}{\sqrt{n}} e^{-n},
\end{equation*}
where $p$ is the number of possible split locations and $n$ the number of observations on the current node.  

\vskip .5cm So things are pretty stable (until they aren't).
\end{frame}

\begin{frame}

{\bf California Housing Data}

\vskip .5cm

\includegraphics[width=\textwidth]{../graphs/ca_trunk}

\vskip .5cm

\hfill 20k observations.

\end{frame}

\begin{frame}

\begin{columns}

\begin{column}{2.15in}
\includegraphics[width=2.5in]{../graphs/ca_splits}
\end{column}

\begin{column}{2in}
\begin{itemize}
\item sample tree occurs 62\% \\of the time.  
\vskip .5cm
\item 90\% of  trees split on income twice, \\and then latitude. 

\vskip .5cm
\item 100\% of trees have 1st 2 splits on median income.  
\end{itemize}
\end{column}

\end{columns}


\vskip 1cm
~~~~~~~~~~~~~~~~~~So trees are stable, at the trunk.
\vskip -1cm

\end{frame}

\begin{frame}

{\bf A big data problem}

\vskip .5cm
RFs are expensive when data is too big to fit in memory.

\vskip .5cm
Subsampling forests (fitting CART on {\it without replacement} samples)
leads to a big drop in performance.

\vskip .5cm
{\theme But wait:}  if the trunks are stable, can we just fit that once and then fit forests at each branch?

\vskip .5cm
This is classic Empirical Bayes: fix higher levels in a {\it hierarchical model}, and direct your machinery at learning the hard bits.

\end{frame}

\begin{frame}

{\bf Emperical Bayesian Forests}

\vskip .5cm
\includegraphics[width=.95\textwidth]{../graphs/ca_rmse}

\vskip .5cm
Since the trunks in a full forest are all similar, EBF and BF give nearly the same results.  {\it SSF does not.}


\end{frame}


\begin{frame}

{\bf EBFs work all over the place}

\vskip .5cm
\begin{minipage}{0.5\linewidth}
\includegraphics[width=\textwidth]{../graphs/beer}
\end{minipage}
~~
\begin{minipage}{0.4\linewidth}
{\footnotesize
\begin{tabular}{c c | l}
$\overline{\text{MCR}}$  & \% WTB & \\
\cline{1-2}\rule{0pt}{3ex} 
0.4341 &  0.0 & BF \\
0.4531 &  4.4 & EBF \\
0.5989 & 38.0 & SSF \\
0.6979 & 60.8 & DT \\
\end{tabular}}
\end{minipage}

\vskip .5cm
\hfill Predicting beer choice from demographics

\end{frame}

\begin{frame}

{\bf EBFs work all over the place}

\vskip .5cm
\begin{minipage}{0.5\linewidth}
\includegraphics[width=\textwidth]{../graphs/wine}
\end{minipage}
~~
\begin{minipage}{0.4\linewidth}
{\footnotesize
\begin{tabular}{c c | l}
$\overline{\text{RMSE}}$  & \% WTB & \\
\cline{1-2}\rule{0pt}{3ex} 
0.5905 &  0.0 & BF \\
0.5953 &  0.8 & EBF \\
0.6607 & 11.9 & SSF \\
0.7648 & 29.5 & DT \\
\end{tabular}}
\end{minipage}

\vskip .5cm
\hfill or wine rating from chemical profile

\end{frame}

\begin{frame}


{\bf \theme EBFs at EBay: \bk predicting Bad Buyer Experiences}

\vskip .5cm
A BBE could be receiving something that is `significantly not as described',
or shipping delays, or any of many other problems.

\vskip .5cm
The models are updated frequently, and  information\\ about $\mathrm{p}(\text{BBE})$
is an input to search rankings  and more.

\vskip .5cm
{\nv Big Data axiom}: more data beats fancy model.

\vskip .2cm The best thing to improve predictions is more data.  \\With millions of daily transactions, there's little limit on data.

\end{frame}

\begin{frame}

{\bf EBFs at EBay}

\vskip .5cm
Full random forest runs take too long on full data \\(even using distributed tree algorithms).

\vskip .5cm
Subsampling led to a noticeable and big drop in performance.

\vskip .5cm
So: EBFs!
\begin{itemize}
\item trunk can be fit in distribution using Spark \texttt{MLLib}.
\item this trunk  acts as a sorting function to map observations \\to 
separate locations corresponding to each branch.
\item Forests are then fit on a machine for each branch.  
\end{itemize}

\end{frame}

\begin{frame}

{\bf EBFs at EBay}

\vskip .5cm

On 12 million transactions,  EBF with 32 branches yields a\\ 1.3\% drop in misclassification over the SSF alternatives.  

\vskip .5cm
This amounts to more than 20,000 extra detected \\BBE occurrences over this short time window.

\vskip .5cm 
Putting it into production requires some careful engineering, \\but this really is {\it a very simple algorithm}.  

\vskip .5cm
If you already fit RFs and are hitting time/space constraints,\\ then an EBF is lots of gain for little pain.

\end{frame}

\begin{frame}

{\theme\bf  The key to big data} 

\vskip .5cm Use plug-in estimates for the stuff that is easy to measure.

\vskip .15cm Partition conditional on these plug-ins.

\vskip .15cm   Direct the full data towards the stuff that is tough to learn.

\vskip 1.5cm
\begin{center}
\Huge
Thanks!
\end{center}

\end{frame}

\end{document}






























